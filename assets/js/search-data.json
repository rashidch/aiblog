{
  
    
        "post0": {
            "title": "Regularized Linear Model (Polynomial Basis) Regression",
            "content": "import numpy as np import matplotlib.pyplot as plt from collections import OrderedDict %matplotlib inline . def generate_data(init, end, num_points, plot=1): &quot;&quot;&quot; x_pts : 1-d array, shape = [num_points,] y_pts : 1-d array, shape = [num_points,] &quot;&quot;&quot; #declare a list of coefficients coeff=[5,1,1] noise=2 # generate x_coordinate of line x = np.arange(init,end,(end-init)/num_points) line = coeff[0] #generate y_coordinates of line for idx in np.arange(1, len(coeff)): line += coeff[idx]*x**idx if noise&gt;0: y= np.random.normal(-(10**noise), 10**noise, len(x))+line else: y= line if plot==1: plt.figure(figsize=(10,6)) plt.scatter(x, y, color=&#39;r&#39;) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) return x, y . x,y=generate_data(0,50,100) . def hypothesis(x, theta): &quot;&quot;&quot; h(x) = theta_0*(x_1**0) + theta_1*(x_1**1) + ...+ theta_n*(x_1 ** n) &quot;&quot;&quot; h = theta[0] for i in np.arange(1,len(theta)): h+= theta[i]*x**i return h . def swapRows( M,i,j): &quot;&quot;&quot; Swaping: - swapRows(M,i,j). Swaps rows i and j of a vector or matrix [v]. &quot;&quot;&quot; if len(M.shape) == 1: M[i],M[j] = M[j],M[i] else: M[[i,j],:] = M[[j,i],:] def swapCols(M,i,j): &quot;&quot;&quot; Swaping: - swapCols(M,i,j). Swaps columns of matrix [M]. &quot;&quot;&quot; M[:,[i,j]] = M[:,[j,i]] def LUdecomp(a,tol=1.0e-9): n = len(a) seq = np.array(range(n)) # Set up scale factors s = np.zeros((n)) for i in range(n): s[i] = max(abs(a[i,:])) for k in range(0,n-1): # Row interchange, if needed p = np.argmax(np.abs(a[k:n,k])/s[k:n]) + k if abs(a[p,k]) &lt; tol: print(&#39;Matrix is singular&#39;) if p != k: swapRows(s,k,p) swapRows(a,k,p) swapRows(seq,k,p) # Elimination for i in range(k+1,n): if a[i,k] != 0.0: lam = a[i,k]/a[k,k] a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n] a[i,k] = lam return a, seq def LUsolve(a,b,seq): &quot;&quot;&quot; Solve Linear equations - x = LUsolve(a,b) Solution phase: solves [L][U]{x} = {b} &quot;&quot;&quot; n = len(a) # Rearrange constant vector; store it in [x] x = b.copy() for i in range(n): #print(seq[i]) x[i] = b[seq[i]] # Solution for k in range(1,n): x[k] = x[k] - np.dot(a[k,0:k],x[0:k]) x[n-1] = x[n-1]/a[n-1,n-1] for k in range(n-2,-1,-1): x[k] = (x[k] - np.dot(a[k,k+1:n],x[k+1:n]))/a[k,k] return x def matInv(a): &quot;&quot;&quot; Calacute inverse of matrix a: - input matrix:a retrun: aInv &quot;&quot;&quot; n = len(a[0]) aInv = np.identity(n) a,seq = LUdecomp(a) for i in range(n): aInv[:,i] = LUsolve(a,aInv[:,i],seq) return aInv . def calculate_error(theta, x, y): m = len(y) h = hypothesis(x, theta) errors = h-y return (1/(2*m))*np.sum(errors**2) . def print_line(theta): fit_line=&#39;&#39; fit_line = fit_line+str(theta[0]) for i in np.arange(1, len(theta)): fit_line = fit_line+&#39; + &#39; + str(theta[i]) + r&#39;x^&#39; + str(i) print(&quot;Fitting Line::::&quot;,fit_line) . def fit_NormalEq(lamda, order, x,y): &quot;&quot;&quot; Fit theta to the training data to the model of the form: h(x) = theta_0*(x_1**0) + theta_1*(x_1**1) + ...+ theta_n*(x_1 ** n) &quot;&quot;&quot; d = {} d[&#39;x&#39; + str(0)] = np.ones([1,len(x)])[0] for i in np.arange(1, order+1): d[&#39;x&#39; + str(i)] = x ** (i) d = OrderedDict(sorted(d.items(), key=lambda t: t[0])) X = np.column_stack(d.values()) print(&#39;X matrix dimension&#39;, X.shape) I = np.eye(order+1)*lamda*order theta = np.matmul(np.matmul(matInv(np.matmul(np.transpose(X),X)+I), np.transpose(X)), y) return theta . def fit_newtonMT(lamda,order,x,y): theta = np.ones(order+1) d={} d[&#39;x&#39;+str(0)]= np.ones([1,len(x)])[0] for i in range(1,order+1): d[&#39;x&#39;+str(i)]= x**i d= OrderedDict(sorted(d.items(),key=lambda t :t[0])) X = np.column_stack(d.values()) I = np.eye(order+1)*lamda while True: a = matInv(np.matmul(np.transpose(X),X) + I) b = np.matmul(np.matmul(np.transpose(X),X),theta) c = np.matmul(np.transpose(X),y) d = np.matmul(I,theta) new_theta = theta - np.matmul(a,(b - c + d)) if abs(calculate_error(new_theta, x, y) - calculate_error(theta, x, y)) &lt; 0.0001: break theta = new_theta return new_theta . def plot_predictedPolyLine(theta,x,y): plt.figure(figsize=(10,6)) plt.scatter(x, y, s = 30, c = &#39;r&#39;) line = theta[0] #y-intercept label_holder = [] label_holder.append(&#39;%.*f&#39; % (2, theta[0])) for i in np.arange(1, len(theta)): line += theta[i] * x ** i label_holder.append(&#39; + &#39; +&#39;%.*f&#39; % (2, theta[i]) + r&#39;$x^&#39; + str(i) + &#39;$&#39;) plt.plot(x, line, label = &#39;&#39;.join(label_holder), color=&#39;g&#39;) plt.title(&#39;Polynomial Fit: Order &#39; + str(len(theta)-1)) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.legend(loc = &#39;best&#39;) . print(&quot;*******Demo of Polynomial Regression********&quot;) print(&quot; &quot;) order = input(&quot;Input the number of polynomial bases: &quot;) order = int(order) lamda = input(&quot;Input the lambda value for regularization: &quot;) lamda = float(lamda) print(&quot;&quot;) print(&quot; &quot;) theta = fit_newtonMT(lamda,order,x,y) error = calculate_error(theta,x,y) print(&quot;Newton Method:&quot;) print_line(theta) print(&quot; &quot;) print(&quot;Prediction error:&quot;, error) print(&quot;&quot;) . *******Demo of Polynomial Regression******** Input the number of polynomial bases: 4 Input the lambda value for regularization: 6 Newton Method: Fitting Line:::: -45.83591013665996 + -9.970455748787613x^1 + 1.5612373069259553x^2 + -0.008712843711862034x^3 + 2.4201524739225014e-05x^4 Prediction error: 4954.219152006292 . C: Users rashi anaconda3 envs flow_14 lib site-packages ipykernel_launcher.py:13: FutureWarning: arrays to stack must be passed as a &#34;sequence&#34; type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future. del sys.path[0] . plot_predictedPolyLine(theta,x,y) . print(&quot;*******Demo of Polynomial Regression********&quot;) print(&quot; &quot;) order = input(&quot;Input the number of polynomial bases: &quot;) order = int(order) lamda = input(&quot;Input the lambda value for regularization: &quot;) lamda = float(lamda) print(&quot;&quot;) print(&quot; &quot;) theta = fit_NormalEq(lamda,order,x,y) error = calculate_error(theta,x,y) print(&quot;LSE:&quot;) print_line(theta) print(&quot; &quot;) print(&quot;Prediction error:&quot;, error) print(&quot;&quot;) . *******Demo of Polynomial Regression******** Input the number of polynomial bases: 5 Input the lambda value for regularization: 7 X matrix dimension (100, 6) LSE: Fitting Line:::: -15.800709930519798 + -7.858887568115296x^1 + 0.4020051171386698x^2 + 0.07514825478930498x^3 + -0.002154536774397562x^4 + 1.8984328814073015e-05x^5 Prediction error: 5043.317746187734 . C: Users rashi anaconda3 envs flow_14 lib site-packages ipykernel_launcher.py:18: FutureWarning: arrays to stack must be passed as a &#34;sequence&#34; type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future. . plot_predictedPolyLine(theta,x,y) .",
            "url": "https://rashidch.github.io/aiblog/2020/11/20/polynomial-regression.html",
            "relUrl": "/2020/11/20/polynomial-regression.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Implementation of Naive Bayes classifier and Online Learning using Numpy",
            "content": "1. Naive Bayes classifier for MNIST . Step by step implementation of a Naive Bayes classifier that support discrete and continuous features for each handwritten digit. | Input: We will use image training data from MNIST Download the MNIST from this website and parse it. | Please read the description in the link to understand the format. | Basically, each image is represented by bits (Whole binary file is in big endian format; we need to deal with it), we can use a char arrary to store an image. | There are some headers we need to deal with as well, please read the link for more details. | . | Training lable data from MNIST. | Testing image from MNIST | Testing label from MNIST | Toggle option 0: discrete mode 1: continuous mode | | Output: we print out the posterior (in log scale to avoid underflow) of the ten categories (0-9) for each image input. | We have to marginalize posterior so that its sum is equal to 1. | For each test image, we print out our prediction which is the category having the highest posterior, and tally the prediction by comparing with input. | Then we print out the imagination of numbers in Bayes classifier. | We calculate and report the error rate in the end. | | Functionality: In Discrete mode: Tally the frequency of the values of each pixel into 32 bins. For example, pixel 0 to 7 should be classified to bin 1, pixel 8 to 15 should be bin 2 ... etc. Then perform Naive Bayes classifier. Note that to avoid empty bin, you can use a peudocount (such as the minimum value in other bins) for instead. | . | In Continuous mode: Use MLE to fit a Gaussian distribution for the value of each pixel. Perform Naive Bayes classifier. | . | | Follow chapter-2 of Pattern Recognition and Machine learning book to understand theory of Naive Bayes classifier from Bayesian perspective. | . import numpy as np from functools import reduce %matplotlib inline from functools import reduce import matplotlib.pyplot as plt from math import log, sqrt, exp, pi, factorial, erf . class load_file(): def __init__(self, file): self.filename = file self.dset = [] with open(self.filename, mode=&#39;rb&#39;) as f: magic_number = f.read(4) # read first four bytes self.n_dim = magic_number[-1] #n_dim=3 self.n_dims = [int.from_bytes(f.read(4), byteorder=&#39;big&#39;) for _ in range(self.n_dim)] #n_dims=[60000,28,28] #read a vector of size 784 for each iteration for idx in range(self.n_dims[0]): item = f.read(reduce(lambda x,y: x*y, (self.n_dims[1:] + [1]))) self.dset.append(item) . file_1 = &#39;./project-02/train-images.idx3-ubyte&#39; file_2 = &#39;./project-02/train-labels.idx1-ubyte&#39; file_3 = &#39;./project-02/t10k-images.idx3-ubyte&#39; file_4 = &#39;./project-02/t10k-labels.idx1-ubyte&#39; . train_x = load_file(file_1) train_y = load_file(file_2) test_x = load_file(file_3) test_y = load_file(file_4) . len(train_x.dset), len(train_y.dset), len(test_x.dset), len(test_y.dset) . (60000, 60000, 10000, 10000) . def showImage(byte, row = 28, col = 28): im = [[byte[i*28 + j] for j in range(col)] for i in range(row)] plt.imshow(im, cmap=&#39;gray&#39;) . data_idx = 10 showImage(train_x.dset[data_idx]) print(&quot;labels [{}] : {}&quot;.format(data_idx, train_y.dset[data_idx][0])) . labels [10] : 3 . Discrete . def cnt_pixclass(train_x, train_y, w = 28, h = 28, nc = 10, bins = 10**-6): pixels_cnt = [[[bins for _ in range(32)] for _ in range(w * h)] for _ in range(nc)] classes_cnt = [0 for _ in range(nc)] for indx, data in enumerate(train_x): classes_cnt[train_y[indx][0]] += 1 for i in range(len(data)): pixels_cnt[train_y[indx][0]][i][data[i] &gt;&gt; 3] += 1 return classes_cnt, pixels_cnt . def fit_posterior_disc(test_x, test_y, classes_cnt, pixels_cnt): posterior = [[0 for _ in range(10)] for _ in range(len(test_x))] accuracy = 0 for indx, data in enumerate(test_x): for y in range(10): for i in range(len(data)): posterior[indx][y] += log(pixels_cnt[y][i][data[i] &gt;&gt; 3]) posterior[indx][y] -= (len(data) - 1 ) * log(classes_cnt[y]) #posterior[idx][label] += log(cnt_classes[label]) pred = np.argmax(posterior[indx]) answ = test_y[indx][0] if pred == answ: accuracy += 1 print(&quot;Accuracy on test images:&quot;, accuracy/len(test_y)) return posterior, accuracy . def predict(test_labels, posterior,indx): p=np.array(posterior[indx])/np.array(posterior[indx]).sum() print(&#39;Posterior (in log scale): n&#39;) for key in (range(0,len(p))): print(key,&#39;:&#39;,p[key]) #print marginalized posterior print(&quot;index: {} t label : {} t predict : {}&quot;.format(indx, test_y.dset[indx][0], np.argmax(posterior[indx]))) . def imagination_discrete(count_pixels,label=0): pixel_sum_015=0 pixel_sum_031=0 img_num =[0 for _ in range (28*28)] for i in range (784): for k in range(32): if k&lt;16: pixel_sum_015=pixel_sum_015+count_pixels[label][i][k] else: pixel_sum_031=pixel_sum_031+count_pixels[label][i][k] if pixel_sum_015&gt;pixel_sum_031: img_num[i]=0 pixel_sum_015=0 else: img_num[i]=1 pixel_sum_031=0 img_num=np.array(img_num).reshape(28,28) print(img_num) #plt.imshow(img_num) . Run Discrete Naive Bayes . cnt_classes, cnt_pixels = cnt_pixclass(train_x.dset, train_y.dset) posterior, accuracy = fit_posterior_disc(test_x.dset, test_y.dset, cnt_classes, cnt_pixels) . Accuracy on test images: 0.8508 . predict(test_y, posterior,indx=6) . Posterior (in log scale): 0 : 0.10791986716830107 1 : 0.14022635805906106 2 : 0.10101108316185439 3 : 0.0934057462599313 4 : 0.0829546091731439 5 : 0.0885178550027136 6 : 0.11456577642271916 7 : 0.09194392970975554 8 : 0.09068244277920456 9 : 0.08877233226331545 index: 6 label : 4 predict : 4 . imagination_discrete(cnt_pixels,label=3) . [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] . &quot;Discrete Error rate : {}&quot;.format(1-(accuracy/len(test_x.dset))) . &#39;Discrete Error rate : 0.1492&#39; . Continous . def extract_pixels(train_x,train_y): pixels = [[[] for _ in range(28*28)] for _ in range(10)] for indx, data in enumerate(train_x): for i in range(len(data)): pixels[train_y[indx][0]][i].append(data[i]) return pixels def normal(u, v): return lambda x : (exp(-1*(((x - u)**2) / (2*v) ) ) / sqrt(2*pi*v)) def pdf1(u, v): return lambda x : 0.5*(1 + erf((x - u)/(sqrt(2*v)))) def pdf(x, w, cdf): a = cdf(x - w/2) b = cdf(x + w/2) return b - a . def sample_u_v(pixels): pixel_normal = [[[0, 0] for _ in range(28*28)] for _ in range(10)] var = 10**3 for i in range(10): for j in range(28*28): n = len(pixels[i][j]) u = sum(pixels[i][j]) / n v = sum([(x - u)**2 for x in pixels[i][j]]) / n pixel_normal[i][j][0] = u if v &lt;= var: v = var pixel_normal[i][j][1] = v pixel_normal[i][j].append(normal(u, v)) pixel_normal[i][j].append(pdf1(u, v)) return pixel_normal . def fit_post_cont(test_x, test_y, pixel_normal, cnt_classess): accuracy = 0 width = 1 ln0 = 10**-10 posterior = [[0 for _ in range(10)] for _ in range(len(test_x))] for indx, data in enumerate(test_x): for k in range(10): for i in range(28*28): p = pdf(data[i], width, pixel_normal[k][i][3]) if p &lt;= ln0: posterior[indx][k] += log(ln0) continue posterior[indx][k] += log(p) posterior[indx][k] += log(cnt_classes[k]) pred = np.argmax(posterior[indx]) answ = test_y[indx][0] if pred == answ: accuracy += 1 return posterior, accuracy . def imagination_continous(pixels,label=3): pixel_mean=0 img_num =[0 for _ in range (28*28)] for i in range (784): pixel_mean =pixels[label][i][0] if pixel_mean&lt;=127: img_num[i]=0 pixel_mean=0 else: img_num[i]=1 pixel_mean=0 img_num=np.array(img_num).reshape(28,28) print(img_num) . Run continous Naive Bayes . pixels=extract_pixels(train_x.dset,train_y.dset) pixel_normal=sample_u_v(pixels) . poster, accuracy1 =fit_post_cont(test_x.dset, test_y.dset, pixel_normal, cnt_classes) . &quot;Continuous accuracy: {}&quot;.format(accuracy/len(test_y.dset)) . &#39;Continuous accuracy: 0.8508&#39; . predict(test_y.dset, poster,indx=9) . Posterior (in log scale): 0 : 0.10306149487014399 1 : 0.11229989126971714 2 : 0.10233893586749167 3 : 0.10202525967813285 4 : 0.0956631808355246 5 : 0.09863294727661963 6 : 0.10240148473075054 7 : 0.09402389601933993 8 : 0.09638031611014225 9 : 0.09317259334213739 index: 9 label : 9 predict : 9 . imagination_continous(pixel_normal,label=9) . [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] . &quot;Discrete Error rate : {}&quot;.format(1-(accuracy1/len(test_y.dset))) . &#39;Discrete Error rate : 0.17000000000000004&#39; . 2. Online Learning . We use online learning to learn the beta distribution of the parameter p (chance to see 1) of the coin tossing trails in batch. | Input: A file (tosscoin.txt) contains many lines of binary outcomes: | Parameter a for the initial beta prior | Parameter b for the initial beta prior | | Output: print out the Binomial likelihood (based on MLE), Beta prior and posterior probability (parameters only) for each line. | Functionality: Use Beta-Binomial conjugation to perform online learning. | . tosscoin=[] filename=&#39;./project-02/tosscoin.txt&#39; with open(filename,&#39;r&#39;) as f: tosses=f.readlines() for trail in tosses: N=len(trail[:-1]) m=0 for i in range(len(trail)): if trail[i]==&#39;1&#39;: m+=1 tosscoin.append((N,m)) print(tosscoin) . [(22, 13), (7, 5), (12, 9), (41, 37), (22, 13), (7, 4), (12, 7), (123, 111), (52, 28), (45, 26), (48, 22), (43, 22), (43, 21), (59, 32), (38, 16), (46, 23), (38, 15), (56, 27), (39, 18), (33, 16), (37, 21), (55, 30), (71, 36), (44, 21), (58, 23), (66, 32), (62, 36)] . def online_learning(tosscoin, init_a = 0, init_b = 0): beta_a = init_a beta_b = init_b print(&quot;Prior(a,b) : ({},{})&quot;.format(beta_a, beta_b)) for idx, (N,m) in enumerate(tosscoin): prior_a = beta_a prior_b = beta_b beta_a += m beta_b += N-m print(&quot;Binomial likelihood: {:.3f} t Prior(a,b): ({},{}) t Posterior(a,b): ({},{})&quot;.format(m/N, prior_a, prior_b, beta_a, beta_b)) . online_learning(tosscoin, init_a=0, init_b=0) . Prior(a,b) : (0,0) Binomial likelihood: 0.591 Prior(a,b): (0,0) Posterior(a,b): (13,9) Binomial likelihood: 0.714 Prior(a,b): (13,9) Posterior(a,b): (18,11) Binomial likelihood: 0.750 Prior(a,b): (18,11) Posterior(a,b): (27,14) Binomial likelihood: 0.902 Prior(a,b): (27,14) Posterior(a,b): (64,18) Binomial likelihood: 0.591 Prior(a,b): (64,18) Posterior(a,b): (77,27) Binomial likelihood: 0.571 Prior(a,b): (77,27) Posterior(a,b): (81,30) Binomial likelihood: 0.583 Prior(a,b): (81,30) Posterior(a,b): (88,35) Binomial likelihood: 0.902 Prior(a,b): (88,35) Posterior(a,b): (199,47) Binomial likelihood: 0.538 Prior(a,b): (199,47) Posterior(a,b): (227,71) Binomial likelihood: 0.578 Prior(a,b): (227,71) Posterior(a,b): (253,90) Binomial likelihood: 0.458 Prior(a,b): (253,90) Posterior(a,b): (275,116) Binomial likelihood: 0.512 Prior(a,b): (275,116) Posterior(a,b): (297,137) Binomial likelihood: 0.488 Prior(a,b): (297,137) Posterior(a,b): (318,159) Binomial likelihood: 0.542 Prior(a,b): (318,159) Posterior(a,b): (350,186) Binomial likelihood: 0.421 Prior(a,b): (350,186) Posterior(a,b): (366,208) Binomial likelihood: 0.500 Prior(a,b): (366,208) Posterior(a,b): (389,231) Binomial likelihood: 0.395 Prior(a,b): (389,231) Posterior(a,b): (404,254) Binomial likelihood: 0.482 Prior(a,b): (404,254) Posterior(a,b): (431,283) Binomial likelihood: 0.462 Prior(a,b): (431,283) Posterior(a,b): (449,304) Binomial likelihood: 0.485 Prior(a,b): (449,304) Posterior(a,b): (465,321) Binomial likelihood: 0.568 Prior(a,b): (465,321) Posterior(a,b): (486,337) Binomial likelihood: 0.545 Prior(a,b): (486,337) Posterior(a,b): (516,362) Binomial likelihood: 0.507 Prior(a,b): (516,362) Posterior(a,b): (552,397) Binomial likelihood: 0.477 Prior(a,b): (552,397) Posterior(a,b): (573,420) Binomial likelihood: 0.397 Prior(a,b): (573,420) Posterior(a,b): (596,455) Binomial likelihood: 0.485 Prior(a,b): (596,455) Posterior(a,b): (628,489) Binomial likelihood: 0.581 Prior(a,b): (628,489) Posterior(a,b): (664,515) . online_learning(tosscoin, init_a=10, init_b=1) . Prior(a,b) : (10,1) Binomial likelihood: 0.591 Prior(a,b): (10,1) Posterior(a,b): (23,10) Binomial likelihood: 0.714 Prior(a,b): (23,10) Posterior(a,b): (28,12) Binomial likelihood: 0.750 Prior(a,b): (28,12) Posterior(a,b): (37,15) Binomial likelihood: 0.902 Prior(a,b): (37,15) Posterior(a,b): (74,19) Binomial likelihood: 0.591 Prior(a,b): (74,19) Posterior(a,b): (87,28) Binomial likelihood: 0.571 Prior(a,b): (87,28) Posterior(a,b): (91,31) Binomial likelihood: 0.583 Prior(a,b): (91,31) Posterior(a,b): (98,36) Binomial likelihood: 0.902 Prior(a,b): (98,36) Posterior(a,b): (209,48) Binomial likelihood: 0.538 Prior(a,b): (209,48) Posterior(a,b): (237,72) Binomial likelihood: 0.578 Prior(a,b): (237,72) Posterior(a,b): (263,91) Binomial likelihood: 0.458 Prior(a,b): (263,91) Posterior(a,b): (285,117) Binomial likelihood: 0.512 Prior(a,b): (285,117) Posterior(a,b): (307,138) Binomial likelihood: 0.488 Prior(a,b): (307,138) Posterior(a,b): (328,160) Binomial likelihood: 0.542 Prior(a,b): (328,160) Posterior(a,b): (360,187) Binomial likelihood: 0.421 Prior(a,b): (360,187) Posterior(a,b): (376,209) Binomial likelihood: 0.500 Prior(a,b): (376,209) Posterior(a,b): (399,232) Binomial likelihood: 0.395 Prior(a,b): (399,232) Posterior(a,b): (414,255) Binomial likelihood: 0.482 Prior(a,b): (414,255) Posterior(a,b): (441,284) Binomial likelihood: 0.462 Prior(a,b): (441,284) Posterior(a,b): (459,305) Binomial likelihood: 0.485 Prior(a,b): (459,305) Posterior(a,b): (475,322) Binomial likelihood: 0.568 Prior(a,b): (475,322) Posterior(a,b): (496,338) Binomial likelihood: 0.545 Prior(a,b): (496,338) Posterior(a,b): (526,363) Binomial likelihood: 0.507 Prior(a,b): (526,363) Posterior(a,b): (562,398) Binomial likelihood: 0.477 Prior(a,b): (562,398) Posterior(a,b): (583,421) Binomial likelihood: 0.397 Prior(a,b): (583,421) Posterior(a,b): (606,456) Binomial likelihood: 0.485 Prior(a,b): (606,456) Posterior(a,b): (638,490) Binomial likelihood: 0.581 Prior(a,b): (638,490) Posterior(a,b): (674,516) .",
            "url": "https://rashidch.github.io/aiblog/2020/11/20/Naive-Bayes-classifier-for-MNIST.html",
            "relUrl": "/2020/11/20/Naive-Bayes-classifier-for-MNIST.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://rashidch.github.io/aiblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rashidch.github.io/aiblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rashidch.github.io/aiblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rashidch.github.io/aiblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}